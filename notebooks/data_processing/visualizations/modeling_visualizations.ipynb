{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3db5ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from bertopic import BERTopic \n",
    "from transformers import *\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath('..')))\n",
    "from utils.text_analysis_functions import data_cleaning\n",
    "from utils.modeling_helpers import summarize_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5af08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bertopic_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))) + \"\\\\notebooks\\\\data_processing\\\\modeling\\\\BERTopic_model\"\n",
    "exploded_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))) + \"\\\\notebooks\\\\data_processing\\\\modeling\\\\exploded_chunks.pkl\"\n",
    "data_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))) + \"\\\\working_data\\\\transformed_dataset.csv\"\n",
    "labeled_data_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))) + \"\\\\working_data\\\\labeled_dataset.csv\"\n",
    "embeddings_path = os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd()))) + \"\\\\working_data\\\\my_data_embeddings.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a9b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"nlpaueb/bert-base-greek-uncased-v1\",\n",
    "    use_fast=True)\n",
    "\n",
    "cleaning_object = data_cleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c502ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leoni\\AppData\\Local\\Temp\\ipykernel_10444\\854892483.py:17: DeprecationWarning:\n",
      "\n",
      "DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_exploded = pd.read_pickle(exploded_path)\n",
    "embeddings = np.load(embeddings_path)\n",
    "\n",
    "# load model & transform\n",
    "topic_model = BERTopic.load(bertopic_path)\n",
    "topics, probs = topic_model.transform(\n",
    "    data_exploded[\"chunks\"],\n",
    "    embeddings=embeddings\n",
    ")\n",
    "data_exploded[\"topic\"] = topics\n",
    "data_exploded[\"topic_prob\"] = [p.max() for p in probs]\n",
    "\n",
    "# summarize per doc\n",
    "doc_topics = (\n",
    "    data_exploded\n",
    "      .groupby(\"doc_id\")\n",
    "      .apply(lambda grp: summarize_doc(grp, topic_model))\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# merge back into the original raw data\n",
    "data = pd.read_csv(data_path)\n",
    "data[\"doc_id\"] = data.index\n",
    "data_labeled  = data.merge(doc_topics, on=\"doc_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d6a276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labeled.to_csv(labeled_data_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387413b1",
   "metadata": {},
   "source": [
    "### Load the labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c35c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = pd.read_csv(labeled_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1297bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c128dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9293"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dataset[(my_dataset[\"dominant_topic\"] == -1) | (my_dataset[\"dominant_topic\"].isna())] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e10566e",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07262b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "my_dataset[my_dataset[\"dominant_topic\"] != -1][\"dominant_topic\"].value_counts().plot(ax=ax, kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caa6881",
   "metadata": {},
   "source": [
    "Confident topic prediction greater than 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "696ecc25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5166289687526179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dataset[my_dataset[\"topic_prob\"] >= 0.82]) / len(my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b537f",
   "metadata": {},
   "source": [
    "### Thematic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20c54d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs = my_dataset[[\"dominant_topic\", \"doc_id\", \"topic_words\"]].groupby([\"dominant_topic\", \"topic_words\"]).size().reset_index(name=\"Count\")\n",
    "unique_pairs[\"Frequency (%)\"] = (unique_pairs[\"Count\"] / 23869) * 100\n",
    "topics_names = [\"Outliers\",\n",
    "                \"Social Roles and Functions\",\n",
    "                \"Divinity as Collective Identity\",\n",
    "                \"Religious conservativism\",\n",
    "                \"Emotionally triggered\",\n",
    "                \"Fear & Panic\",\n",
    "                \"Knowledge Power\",\n",
    "                \"-\",\n",
    "                \"Egalitarian\",\n",
    "                \"-\",\n",
    "                \"-\",\n",
    "                \"Emotionally triggered\",\n",
    "                \"-\",\n",
    "                \"Equality hides in the details\"]\n",
    "\n",
    "unique_pairs[\"Topic name\"] = topics_names\n",
    "unique_pairs = unique_pairs.rename(columns={\"dominant_topic\": \"Topic No\", \"topic_words\": \"Top 10 words\"})\n",
    "unique_pairs[\"Topic No\"] = unique_pairs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d203cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_pairs.to_latex()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db577733",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_merge = unique_pairs[[\"Topic name\",\"Topic No\"]].copy()\n",
    "for_merge[\"Topic No\"] = for_merge[\"Topic No\"] - 1\n",
    "my_dataset_new = my_dataset.merge(for_merge, left_on=\"dominant_topic\", right_on=\"Topic No\", how=\"left\")\n",
    "my_dataset_new.drop(\"Topic No\", axis=1, inplace=True)\n",
    "my_dataset_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = (\n",
    "    my_dataset_new\n",
    "    .loc[my_dataset_new[\"Topic name\"] != \"-\", \"Topic name\"]\n",
    "    .value_counts()\n",
    ")\n",
    "\n",
    "total = counts.sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "counts.plot(kind=\"barh\", ax=ax)\n",
    "\n",
    "for p in ax.patches:\n",
    "    width = p.get_width()\n",
    "    if width == 0:\n",
    "        continue\n",
    "    pct = 100 * width / total\n",
    "    y = p.get_y() + p.get_height() / 2\n",
    "    ax.annotate(f\"{pct:.1f}%\",\n",
    "                (width, y),\n",
    "                ha=\"left\", va=\"center\",\n",
    "                xytext=(3, 0), textcoords=\"offset points\")\n",
    "\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdfba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in unique_pairs.loc[0:15].iterrows():\n",
    "    print(row[\"Topic No\"], row[\"Topic name\"])\n",
    "    print(row[\"Top 10 words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f77a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in list(my_dataset[\"dominant_topic\"].unique()):\n",
    "    if topic != -1 and pd.notna(topic):\n",
    "        topic1_rows = my_dataset[(my_dataset[\"dominant_topic\"] == topic) & (my_dataset[\"topic_prob\"] > 0.95)]\n",
    "        n = min(20, len(topic1_rows))\n",
    "        sampled = topic1_rows[[\"comment_id\", \"text\"]].sample(n=n, random_state=42)\n",
    "        print(f\"Topic {topic} samples:\")\n",
    "        for num, row in sampled.iterrows():\n",
    "            print(f\"{num+1}. (ID: {row['comment_id']}) {row['text']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39698230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1533467370361062"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_s = my_dataset[~my_dataset[\"dominant_topic\"].notna()]\n",
    "minus_ones = my_dataset[my_dataset[\"dominant_topic\"] == -1]\n",
    "valids = my_dataset[(my_dataset[\"dominant_topic\"] != -1) & (my_dataset[\"dominant_topic\"].notna())]\n",
    "len(na_s) / len(my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d409b",
   "metadata": {},
   "source": [
    "### Over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f6577a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path)\n",
    "exploded = data_exploded.copy()\n",
    "data[\"doc_id\"] = data.index\n",
    "exploded = exploded.merge(\n",
    "    data[[\"doc_id\", \"date\", \"date_mini\", \"like_scaled_norm\"]],\n",
    "    on=\"doc_id\",\n",
    "    how=\"left\")\n",
    "assert len(exploded) == embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "49eb1df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_time = exploded[~exploded[\"topic\"].isna()]\n",
    "comments = for_time[\"text\"]\n",
    "timestamps = for_time[\"date\"]\n",
    "topics = for_time[\"topic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b5f89867",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(comments, timestamps, nr_bins=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_TO_SHARE = True \n",
    "\n",
    "df = my_dataset_new.copy()\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "df['Topic name'] = df['Topic name'].fillna('Unknown').astype(str)\n",
    "df = df[~df['Topic name'].isin(['Outliers', '-', 'Unknown'])].copy()\n",
    "\n",
    "df['like_scaled'] = pd.to_numeric(df['like_scaled'], errors='coerce').fillna(0.0)\n",
    "\n",
    "topics_all = sorted(df['Topic name'].unique())\n",
    "cmap = mpl.cm.get_cmap('tab20', len(topics_all))\n",
    "TOPIC_COLORS = {t: cmap(i) for i, t in enumerate(topics_all)}\n",
    "\n",
    "def weighted_topic_over_time(data: pd.DataFrame, freq: str, normalize: bool=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a wide DF: index=time bucket, columns=Topic name, values = sum(like_scaled).\n",
    "    If normalize=True, each row is divided by the row total (shares).\n",
    "    \"\"\"\n",
    "    weights = (\n",
    "        data\n",
    "        .groupby([pd.Grouper(key='date', freq=freq), 'Topic name'])['like_scaled']\n",
    "        .sum()\n",
    "        .unstack(fill_value=0.0)\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    for t in topics_all:\n",
    "        if t not in weights.columns:\n",
    "            weights[t] = 0.0\n",
    "    weights = weights[topics_all] \n",
    "\n",
    "    if normalize:\n",
    "        row_sums = weights.sum(axis=1).replace(0, pd.NA)\n",
    "        weights = weights.div(row_sums, axis=0).fillna(0.0)\n",
    "    return weights\n",
    "\n",
    "def plot_weighted(counts: pd.DataFrame, title: str, normalize: bool=False):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    for t in counts.columns:\n",
    "        ax.plot(counts.index, counts[t], label=t, linewidth=2, color=TOPIC_COLORS[t])\n",
    "    ax.set_title(title, fontsize=16, pad=12)\n",
    "    ax.set_xlabel(\"Date\", fontsize=13)\n",
    "    ax.set_ylabel(\"Share\" if normalize else \"Weighted frequency (sum of like_scaled)\", fontsize=13)\n",
    "    ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.legend(title=\"Topic name\", bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for freq, label in [('YS', 'Yearly'), ('MS', 'Monthly'), ('W-MON', 'Weekly (Mon)')]:\n",
    "    counts = weighted_topic_over_time(df, freq=freq, normalize=NORMALIZE_TO_SHARE)\n",
    "    plot_weighted(counts, f\"Topic {('shares' if NORMALIZE_TO_SHARE else 'weighted frequencies')} — {label}\", normalize=NORMALIZE_TO_SHARE)\n",
    "\n",
    "for period_value, g in df.groupby('period'):\n",
    "    if g.empty:\n",
    "        continue\n",
    "    counts = weighted_topic_over_time(g, freq='W-MON', normalize=NORMALIZE_TO_SHARE)\n",
    "    plot_weighted(counts, f\"Weekly topic {('shares' if NORMALIZE_TO_SHARE else 'weighted frequencies')} — period: {period_value}\", normalize=NORMALIZE_TO_SHARE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
