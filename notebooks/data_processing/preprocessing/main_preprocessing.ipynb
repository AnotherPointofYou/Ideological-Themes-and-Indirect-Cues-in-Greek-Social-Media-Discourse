{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafcf5cc",
   "metadata": {},
   "source": [
    "### __Main preprocessing for all sources__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab9009e",
   "metadata": {},
   "source": [
    "#### __Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8eb03e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leoni\\Desktop\\School\\Thesis\\the_project\\master_thesis_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, json, copy, sys\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath('..')))\n",
    "from utils.helpers import rename_dictionary_keys, assign_unique_author_ids\n",
    "from utils.text_analysis_functions import data_cleaning, filtering_pipelines, cleaning_pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e6fa717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install gr-nlp-toolkit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75033f",
   "metadata": {},
   "source": [
    "#### __Functions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9dbb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scalar_weights(post_scores, comment_counts, scores_min, scores_max, comment_count_min, comment_count_max):\n",
    "    \"\"\"\n",
    "    Compute scalar weights for posts based on normalized post scores and comment counts.\n",
    "    \n",
    "    Args:\n",
    "        - post_scores (array-like): Raw scores for each post.\n",
    "        - comment_counts (array-like): Comment counts for each post.\n",
    "    \n",
    "    Returns:\n",
    "        - numpy.ndarray: Scalar weights for each post, in the range [0, 1].\n",
    "    \"\"\"\n",
    "    p = np.array(post_scores, dtype=float)\n",
    "    c = np.array(comment_counts, dtype=float)\n",
    "    # normalized post scores\n",
    "    p_norm = (p - scores_min) / (scores_max - scores_min) if scores_max != scores_min else np.zeros_like(p)\n",
    "    # normalized comment counts\n",
    "    C_norm = (c - comment_count_min) / (comment_count_max - comment_count_min) if comment_count_max != comment_count_min else np.zeros_like(C)\n",
    "    # final scalar\n",
    "    S = 0.5 * p_norm + 0.5 * C_norm\n",
    "    return S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49989dc2",
   "metadata": {},
   "source": [
    "#### __Data__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a550154",
   "metadata": {},
   "source": [
    "**We check the following:**\n",
    "\n",
    "- ***Reddit posts*** from targeted subreddits\n",
    "- ***YouTube titles*** based on keyword search\n",
    "\n",
    "<ins>Filtering criteria:</ins> Keyword stem appearance\n",
    "\n",
    "**Required Datasets:**\n",
    "\n",
    "- ***YouTube video metadata***: yt_videos\n",
    "- ***YouTube video comments***: yt_comments\n",
    "- ***Reddit post metadata***: rd_posts\n",
    "- ***Reddit post comments***: rd_comments\n",
    "- ***OpenGov unified comments***: ogov_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b6c4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reddit\n",
    "reddit_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(\"main_preprocessing.ipynb\"))))) + \"\\\\outputs\\\\api_queried\\\\reddit_api\"\n",
    "## YouTube\n",
    "youtube_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(\"main_preprocessing.ipynb\"))))) + \"\\\\outputs\\\\api_queried\\\\youtube_api\"\n",
    "## OpenGov \n",
    "opengov_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(\"main_preprocessing.ipynb\"))))) + \"\\\\outputs\\\\site_scraped\\\\same_sex_marriage_law\"\n",
    "\n",
    "## YouTube\n",
    "# videos\n",
    "with open(youtube_path + \"\\\\youtube_scraped_videos.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    yt_videos = json.load(f)\n",
    "## comments\n",
    "youtube_jsons  = [file for file in os.listdir(youtube_path + \"\\\\youtube_comments\") if file.endswith('.json')]\n",
    "youtube_jsons_path = youtube_path + \"\\\\youtube_comments\"\n",
    "youtube_comments = [] # combine opengov .json\n",
    "\n",
    "for filename in youtube_jsons:\n",
    "    filepath = os.path.join(youtube_jsons_path, filename)\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        youtube_comments.extend(data)\n",
    "\n",
    "## OpenGov\n",
    "opengov_files  = [file for file in os.listdir(opengov_path) if file.endswith('.json')]\n",
    "ogov_comments = [] # combine opengov .json\n",
    "for filename in opengov_files:\n",
    "    filepath = os.path.join(opengov_path, filename)\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        ogov_comments.extend(data)\n",
    "\n",
    "## Reddit\n",
    "## posts\n",
    "with open(reddit_path + \"\\\\reddit_scraped_post.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reddit_posts = json.load(f)\n",
    "## comments\n",
    "with open(reddit_path + \"\\\\reddit_scraped_comments.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reddit_comments = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfc9ac1",
   "metadata": {},
   "source": [
    "#### __Preprocessing__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219dc801",
   "metadata": {},
   "source": [
    "##### Object Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = data_cleaning()\n",
    "filtering_pipe = filtering_pipelines()\n",
    "cleaning_pipe = cleaning_pipelines() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a16d579",
   "metadata": {},
   "source": [
    "##### Homogenize dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8425c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_videos_edited = copy.deepcopy(yt_videos)\n",
    "youtube_comments_edited = copy.deepcopy(youtube_comments)\n",
    "reddit_comments_edited = copy.deepcopy(reddit_comments)\n",
    "\n",
    "for idx, vid in enumerate(yt_videos_edited):\n",
    "    rename_dictionary_keys(yt_videos_edited[idx],\"video_id\",\"id\")\n",
    "\n",
    "for idx, vid in enumerate(youtube_comments_edited):\n",
    "    rename_dictionary_keys(youtube_comments_edited[idx],\"video_id\",\"id\")\n",
    "\n",
    "for idx, com in enumerate(reddit_comments_edited):\n",
    "    rename_dictionary_keys(reddit_comments_edited[idx],\"post_id\",\"id\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7defe58",
   "metadata": {},
   "source": [
    "##### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c330dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_ids = []\n",
    "for i in youtube_comments_edited:\n",
    "    yt_ids.append(i[\"id\"])\n",
    "\n",
    "reddit_ids = []\n",
    "for j in reddit_comments_edited:\n",
    "    reddit_ids.append(j[\"id\"])\n",
    "\n",
    "youtube_unique_list = []\n",
    "youtube_non_unique_list = []\n",
    "for id in reddit_ids:\n",
    "    if id not in youtube_unique_list:\n",
    "        youtube_unique_list.append(id)\n",
    "    else:\n",
    "        if id in youtube_non_unique_list:\n",
    "            pass\n",
    "        else:\n",
    "            youtube_non_unique_list.append(id)\n",
    "\n",
    "reddit_unique_list = []\n",
    "reddit_non_unique_list = []\n",
    "for id in reddit_ids:\n",
    "    if id not in reddit_unique_list:\n",
    "        reddit_unique_list.append(id)\n",
    "    else:\n",
    "        if id in reddit_non_unique_list:\n",
    "            pass\n",
    "        else:\n",
    "            reddit_non_unique_list.append(id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "679cfde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comments', 'id'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_comments_edited[5].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a5d9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For YouTube\n",
    "seen_youtube_ids = []\n",
    "filtered_youtube_comments = []\n",
    "\n",
    "for comment in youtube_comments_edited:\n",
    "    comment_id = comment[\"id\"]\n",
    "    if comment_id not in seen_youtube_ids:\n",
    "        filtered_youtube_comments.append(comment)\n",
    "        seen_youtube_ids.append(comment_id)\n",
    "\n",
    "# For Reddit\n",
    "seen_reddit_ids = []\n",
    "filtered_reddit_comments = []\n",
    "\n",
    "for comment in reddit_comments_edited:\n",
    "    comment_id = comment[\"id\"]\n",
    "    if comment_id not in seen_reddit_ids:\n",
    "        filtered_reddit_comments.append(comment)\n",
    "        seen_reddit_ids.append(comment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c7ade32",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_comments_edited = filtered_youtube_comments\n",
    "reddit_comments_edited = filtered_reddit_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0ce5b",
   "metadata": {},
   "source": [
    "##### Filtering content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7665f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_keywords = [\"ομόφυλα ζευγάρια\",\n",
    "                  \"ομόφυλα τεκνοθεσία\",\n",
    "                  \"ισότητα στο πολιτικό γάμο\",\n",
    "                  \"γάμος ομόφυλων\",\n",
    "                  \"γάμος ομόφυλων ζευγαριών\",\n",
    "                  \"ομόφυλα\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92ce658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_buckets = [reddit_comments_edited, youtube_comments_edited]\n",
    "object_buckets = [yt_videos_edited, reddit_posts]\n",
    "\n",
    "valid_ids = []\n",
    "\n",
    "for object_bucket in object_buckets:\n",
    "    for obj in object_bucket:\n",
    "        if filtering_pipe.filter_content(obj[\"title\"], greek_keywords):\n",
    "            valid_ids.append(obj[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b34cc75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_comments_filtered = []\n",
    "youtube_comments_filtered = []\n",
    "blocked = []\n",
    "\n",
    "for idx, comment_platform in enumerate(comment_buckets):\n",
    "    for comment in comment_platform:\n",
    "        if comment[\"id\"] in valid_ids:\n",
    "            if idx == 0:\n",
    "                reddit_comments_filtered.append(comment)\n",
    "            else:\n",
    "                youtube_comments_filtered.append(comment)\n",
    "        else:\n",
    "            blocked.append(comment[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3849ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not printing anything, there are not blocked videos\n",
    "for id in blocked:\n",
    "    for vid in yt_videos_edited:\n",
    "        if vid[\"id\"] == id:\n",
    "            filtering_pipe.filter_content(vid[\"title\"], greek_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d3d6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial YT video count:\", len(youtube_comments_edited), \"Filtered count:\", len(youtube_comments_filtered))\n",
    "print(\"Initial Reddit post count:\", len(reddit_comments_edited), \"Filtered count:\", len(reddit_comments_filtered))\n",
    "print(\"OpenGov comment count\", len(ogov_comments)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9d8c310",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_plain_comments = []\n",
    "for com in youtube_comments_filtered:\n",
    "    for dic in com[\"comments\"]:\n",
    "        youtube_plain_comments.append(dic[\"body\"])\n",
    "\n",
    "ogov_plain_comments = []\n",
    "for com in ogov_comments:\n",
    "    ogov_plain_comments.append(com[\"article_text\"])\n",
    "\n",
    "reddit_plain_comments = []\n",
    "for com in reddit_comments_filtered:\n",
    "    for dic in com[\"comments\"]:\n",
    "        reddit_plain_comments.append(dic[\"body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf45cc",
   "metadata": {},
   "source": [
    "##### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_steps_to_run = [\"normalize\", \"reddit_specific\", \"transliterate\"]\n",
    "yt_steps_to_run = [\"normalize\", \"youtube_specific\", \"transliterate\"]\n",
    "ogov_steps_to_run = [\"normalize\", \"transliterate\"]\n",
    "\n",
    "# YouTube\n",
    "youtube_cleaned = []\n",
    "for video in youtube_comments_filtered:\n",
    "    video_copy = copy.deepcopy(video)\n",
    "    for comment in video_copy[\"comments\"]:\n",
    "        cleaned_text = cleaning_pipe.text_cleaning(comment[\"body\"], yt_steps_to_run)\n",
    "        comment[\"body\"] = cleaned_text\n",
    "    youtube_cleaned.append(video_copy)\n",
    "\n",
    "# Reddit\n",
    "reddit_cleaned = []\n",
    "for post in reddit_comments_filtered:\n",
    "    post_copy = copy.deepcopy(post)\n",
    "    for comment in post_copy[\"comments\"]:\n",
    "        cleaned_text = cleaning_pipe.text_cleaning(comment[\"body\"], reddit_steps_to_run)\n",
    "        comment[\"body\"] = cleaned_text\n",
    "    reddit_cleaned.append(post_copy)\n",
    "\n",
    "# OpenGov\n",
    "ogov_cleaned = []\n",
    "for entry in ogov_comments:\n",
    "    entry_copy = copy.deepcopy(entry)\n",
    "    cleaned_text = cleaning_pipe.text_cleaning(entry_copy[\"article_text\"], ogov_steps_to_run)\n",
    "    entry_copy[\"article_text\"] = cleaned_text\n",
    "    ogov_cleaned.append(entry_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f696071c",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_cleaned = [\n",
    "    {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"comments\": [c for c in item[\"comments\"] if (c.get(\"body\") or \"\").strip()]\n",
    "    }\n",
    "    for item in youtube_cleaned]\n",
    "\n",
    "reddit_cleaned = [\n",
    "    {\n",
    "        \"id\": item[\"id\"],\n",
    "        \"comments\": [c for c in item[\"comments\"] if (c.get(\"body\") or \"\").strip()]\n",
    "    }\n",
    "    for item in reddit_cleaned]\n",
    "\n",
    "ogov_cleaned = [\n",
    "    item for item in ogov_cleaned\n",
    "    if (item.get(\"article_text\") or \"\").strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(youtube_cleaned), len(reddit_cleaned), len(ogov_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79013917",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_map, youtube_cleaned_with_ids, reddit_cleaned_with_ids, ogov_cleaned_with_ids = assign_unique_author_ids(\n",
    "    youtube_cleaned,\n",
    "    reddit_cleaned,\n",
    "    ogov_cleaned\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcacc10",
   "metadata": {},
   "source": [
    "##### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d4e39",
   "metadata": {},
   "source": [
    "Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fbfe8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "like_counts = [item[\"like_count\"] for item in reddit_posts]\n",
    "comment_counts = [item[\"num_comments\"] for item in reddit_posts]\n",
    "\n",
    "scores_min, scores_max = min(like_counts), max(like_counts)\n",
    "comment_count_min, comment_count_max = min(comment_counts), max(comment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3921ba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in reddit_posts:\n",
    "    post[\"popularity_scaler\"] = compute_scalar_weights(\n",
    "    post[\"like_count\"],\n",
    "    post[\"num_comments\"],\n",
    "    scores_min=scores_min,\n",
    "    scores_max=scores_max,\n",
    "    comment_count_min=comment_count_min,\n",
    "    comment_count_max=comment_count_max\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1771331",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_scaler_lookup = {\n",
    "    post[\"id\"]: post[\"popularity_scaler\"]\n",
    "    for post in reddit_posts\n",
    "}\n",
    "\n",
    "for thread in reddit_cleaned_with_ids:\n",
    "    post_id = thread.get(\"id\")\n",
    "    scaler = post_scaler_lookup.get(post_id)\n",
    "    for comment in thread.get(\"comments\", []):\n",
    "        comment[\"popularity_scaler\"] = scaler \n",
    "        # scale likes\n",
    "        like_count = comment.get(\"like_count\", 0)\n",
    "        comment[\"like_scaled\"] = scaler * abs(like_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89b685",
   "metadata": {},
   "source": [
    "YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "153b362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_like_counts = [video[\"like_count\"] for video in yt_videos_edited]\n",
    "yt_comment_counts = [video[\"comment_count\"] for video in yt_videos_edited]\n",
    "\n",
    "yt_scores_min, yt_scores_max = min(yt_like_counts), max(yt_like_counts)\n",
    "yt_comment_count_min, yt_comment_count_max = min(yt_comment_counts), max(yt_comment_counts)\n",
    "\n",
    "for video in yt_videos_edited:\n",
    "    video[\"popularity_scaler\"] = compute_scalar_weights(\n",
    "        video[\"like_count\"],\n",
    "        video[\"comment_count\"],\n",
    "        scores_min=yt_scores_min,\n",
    "        scores_max=yt_scores_max,\n",
    "        comment_count_min=yt_comment_count_min,\n",
    "        comment_count_max=yt_comment_count_max\n",
    "    )\n",
    "\n",
    "yt_scaler_lookup = {\n",
    "    video[\"id\"]: video[\"popularity_scaler\"]\n",
    "    for video in yt_videos_edited\n",
    "}\n",
    "\n",
    "for thread in youtube_cleaned_with_ids:\n",
    "    vid_id = thread.get(\"id\")\n",
    "    scaler = yt_scaler_lookup.get(vid_id, 0)\n",
    "\n",
    "    for comment in thread.get(\"comments\", []):\n",
    "        comment[\"popularity_scaler\"] = scaler\n",
    "        # scale likes\n",
    "        like_count = comment.get(\"like_count\", 0)\n",
    "        comment[\"like_scaled\"] = scaler * like_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfa6fa8",
   "metadata": {},
   "source": [
    "##### Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74514f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(\".\")))) + \"\\\\working_data\"\n",
    "\n",
    "with open(os.path.join(output_path, \"youtube_cleaned.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(youtube_cleaned_with_ids, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(output_path, \"reddit_cleaned.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reddit_cleaned_with_ids, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(output_path, \"ogov_cleaned.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ogov_cleaned_with_ids, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(output_path, \"author_id_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(author_map, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79689a43",
   "metadata": {},
   "source": [
    "@misc{loukas-etal-2025-greek-nlp-toolkit,\n",
    "    title={GR-NLP-TOOLKIT: An Open-Source NLP Toolkit for Modern Greek}, \n",
    "    author={Lefteris Loukas and Nikolaos Smyrnioudis and Chrysa Dikonomaki and Spyros Barbakos and Anastasios Toumazatos and John Koutsikakis and Manolis Kyriakakis and Mary Georgiou and Stavros Vassos and John Pavlopoulos and Ion Androutsopoulos},\n",
    "    year={2025},\n",
    "    eprint={2412.08520},\n",
    "    archivePrefix={arXiv},\n",
    "    primaryClass={cs.CL},\n",
    "    url={https://arxiv.org/abs/2412.08520}, \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
