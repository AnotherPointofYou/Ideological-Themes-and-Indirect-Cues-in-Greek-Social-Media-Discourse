{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb809bc",
   "metadata": {},
   "source": [
    "### Anonymize comments & tabularize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e5380be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leoni\\Desktop\\School\\Thesis\\the_project\\master_thesis_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, json, sys\n",
    "from datetime import datetime, timezone\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath('..')))\n",
    "from utils.text_analysis_functions import data_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4ad645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_greek_date(date_str):\n",
    "    greek_months = {\n",
    "    \"Ιανουαρίου\": \"01\", \"Φεβρουαρίου\": \"02\", \"Μαρτίου\": \"03\", \"Απριλίου\": \"04\",\n",
    "    \"Μαΐου\": \"05\", \"Ιουνίου\": \"06\", \"Ιουλίου\": \"07\", \"Αυγούστου\": \"08\",\n",
    "    \"Σεπτεμβρίου\": \"09\", \"Οκτωβρίου\": \"10\", \"Νοεμβρίου\": \"11\", \"Δεκεμβρίου\": \"12\"\n",
    "    }\n",
    "    try:\n",
    "        day, month_name, rest = date_str.strip().split(\" \", 2)\n",
    "        year = rest.split(\",\")[0].strip()\n",
    "        month = greek_months.get(month_name)\n",
    "        return datetime.strptime(f\"{year}-{month}\", \"%Y-%m\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def assign_length_bin(count):\n",
    "    word_count_bins = {\n",
    "        'short': (0, 25),\n",
    "        'medium': (26, 90),\n",
    "        'long': (91, float('inf'))\n",
    "    }\n",
    "    for label, (low, high) in word_count_bins.items():\n",
    "        if low <= count <= high:\n",
    "            return label\n",
    "    return 'unknown'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014eb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reddit\n",
    "reddit_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(\".\")))) + \"\\\\working_data\\\\reddit_cleaned.json\"\n",
    "## YouTube\n",
    "youtube_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(\".\")))) + \"\\\\working_data\\\\youtube_cleaned.json\"\n",
    "## OpenGov \n",
    "opengov_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(\".\")))) + \"\\\\working_data\\\\ogov_cleaned.json\"\n",
    "\n",
    "## YouTube\n",
    "with open(youtube_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    youtube_clean = json.load(f)\n",
    "## Reddit\n",
    "with open(reddit_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    reddit_clean = json.load(f)\n",
    "## OpenGov\n",
    "with open(opengov_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    ogov_clean = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55499a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_remove_reddit = [\n",
    "    'author',\n",
    "    'reddit_id'\n",
    "]\n",
    "\n",
    "for thread in reddit_clean:\n",
    "    thread_id = thread.get('id', 'empty')\n",
    "    for seq, comment in enumerate(thread.get('comments', []), start=1):\n",
    "        author = comment.get('author_id', 'None')\n",
    "        # random suffix\n",
    "        rand_suffix = uuid.uuid4().hex[:8]\n",
    "        # new comment_id\n",
    "        comment_id = f\"R-{author}-{thread_id}-{seq}-{rand_suffix}\"\n",
    "        comment['unique_comment_id'] = comment_id\n",
    "        # drop sensitive fields\n",
    "        for key in keys_to_remove_reddit:\n",
    "            comment.pop(key, None)\n",
    "\n",
    "keys_to_remove_youtube = [\n",
    "    'author',\n",
    "    'comment_id'\n",
    "]\n",
    "\n",
    "for video in youtube_clean:\n",
    "    video_id = video.get('id', 'empty')\n",
    "    for seq, comment in enumerate(video.get('comments', []), start=1):\n",
    "        author = comment.get('author_id', 'None')\n",
    "        # random suffix\n",
    "        rand_suffix = uuid.uuid4().hex[:8]\n",
    "        # new comment_id\n",
    "        comment_id = f\"Y-{author}-{video_id}-{seq}-{rand_suffix}\"\n",
    "        comment['unique_comment_id'] = comment_id\n",
    "        # drop sensitive fields\n",
    "        for key in keys_to_remove_youtube:\n",
    "            comment.pop(key, None)\n",
    "\n",
    "keys_to_remove_ogov = [\n",
    "    'author_name',\n",
    "    'URL'\n",
    "]\n",
    "\n",
    "for seq, comment in enumerate(ogov_clean, start=1):\n",
    "    author = comment.get('author_id','anon')\n",
    "    # random suffix\n",
    "    suffix = uuid.uuid4().hex[:8]\n",
    "    # new comment_id\n",
    "    comment['unique_comment_id'] = f\"O-{author}-{seq}-{suffix}\"\n",
    "    # drop sensitive fields\n",
    "    for k in keys_to_remove_ogov:\n",
    "        comment.pop(k, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec0d6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "## Reddit\n",
    "for thread in reddit_clean:\n",
    "    for c in thread.get(\"comments\", []):\n",
    "        wc = data_cleaning.word_count(c[\"body\"])\n",
    "        date = datetime.fromisoformat(c[\"published_at\"]).replace(day=1)\n",
    "        records.append({\n",
    "            \"platform\": \"reddit\",\n",
    "            \"date\": date,\n",
    "            \"text\": c[\"body\"],\n",
    "            \"like_count\": c.get(\"like_count\", 0),\n",
    "            \"word_count\": wc,\n",
    "            \"like_scaled\": c.get(\"like_scaled\", 0),\n",
    "            \"comment_id\": c[\"unique_comment_id\"]\n",
    "        })\n",
    "\n",
    "## YouTube\n",
    "for video in youtube_clean:\n",
    "    for c in video.get(\"comments\", []):\n",
    "        wc = data_cleaning.word_count(c[\"body\"])\n",
    "        date = datetime.fromisoformat(c[\"published_at\"]).replace(day=1)\n",
    "        records.append({\n",
    "            \"platform\": \"youtube\",\n",
    "            \"date\": date,\n",
    "            \"text\": c[\"body\"],\n",
    "            \"like_count\": c.get(\"like_count\", 0),\n",
    "            \"word_count\": wc,\n",
    "            \"like_scaled\": c.get(\"like_scaled\", 0),\n",
    "            \"comment_id\": c[\"unique_comment_id\"]\n",
    "        })\n",
    "\n",
    "## OpenGov\n",
    "for c in ogov_clean:\n",
    "    wc = data_cleaning.word_count(c[\"article_text\"])\n",
    "    date = parse_greek_date(c[\"date_published\"].strip())\n",
    "    records.append({\n",
    "        \"platform\": \"opengov\",\n",
    "        \"date\": date,\n",
    "        \"text\": c[\"article_text\"],\n",
    "        \"like_count\": 1,\n",
    "        \"word_count\": wc,\n",
    "        \"like_scaled\": 1/len(ogov_clean),\n",
    "        \"comment_id\": c[\"unique_comment_id\"]\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce', utc=True)\n",
    "df['text_length_bin'] = df['word_count'].apply(assign_length_bin)\n",
    "df['period'] = pd.cut(\n",
    "    df['date'],\n",
    "    bins=[\n",
    "        datetime(2000, 1, 1, tzinfo=timezone.utc),\n",
    "        datetime(2023, 11, 30, tzinfo=timezone.utc),\n",
    "        datetime(2024, 2, 29, tzinfo=timezone.utc),\n",
    "        datetime(2100, 1, 1, tzinfo=timezone.utc)\n",
    "    ],\n",
    "    labels=['pre', 'during', 'post']\n",
    ")\n",
    "df['date_mini'] = df['date'].dt.strftime('%Y-%m')\n",
    "\n",
    "df['like_scaled_norm'] = (\n",
    "    df.groupby('platform')['like_scaled'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6fc983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(\".\")))) + \"\\\\working_data\"\n",
    "\n",
    "with open(os.path.join(output_path, \"youtube_cleaned_anonymized.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(youtube_clean, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(output_path, \"reddit_cleaned_anonymized.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(reddit_clean, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(output_path, \"ogov_cleaned_anonymized.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ogov_clean, f, ensure_ascii=False, indent=2) \n",
    "\n",
    "df.to_csv(output_path + \"\\\\transformed_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
