{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e43f281",
   "metadata": {},
   "source": [
    "## __YouTube Platform: Same-sex marriage__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efed4cc",
   "metadata": {},
   "source": [
    "### __General__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbd721",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee47c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import sys, json, time, os, csv\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from utils.helpers import unique_posts_videos "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ea026",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fe029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_youtube_videos(\n",
    "    youtube_object,\n",
    "    keywords: List[str],\n",
    "    region_code: str = 'GR',\n",
    "    language: str = 'el',\n",
    "    max_results: int = 5,\n",
    "    published_after: str = None,\n",
    "    delay: float = 0\n",
    "    ) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Searches YouTube for multiple keywords and returns video metadata.\n",
    "\n",
    "    Args:\n",
    "        youtube_object: Authenticated YouTube API object.\n",
    "        keywords (List[str]): List of keyword queries.\n",
    "        region_code (str): Region code to localize the results.\n",
    "        language (str): Language code to prefer results in.\n",
    "        max_results (int): Max results per keyword.\n",
    "        published_after (str): ISO 8601 date (e.g. '2024-01-01T00:00:00Z') to filter.\n",
    "        delay (float): Delay in seconds between API calls (rate limiting).\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of video metadata dictionaries.\n",
    "    \"\"\"\n",
    "\n",
    "    all_videos = []\n",
    "\n",
    "    for query in keywords:\n",
    "        try:\n",
    "            # search for videos\n",
    "            search_request = youtube_object.search().list(\n",
    "                part='snippet',\n",
    "                q=query,\n",
    "                type='video',\n",
    "                regionCode=region_code,\n",
    "                relevanceLanguage=language,\n",
    "                maxResults=max_results,\n",
    "                publishedAfter=published_after\n",
    "            )\n",
    "            search_response = search_request.execute()\n",
    "            video_ids = [item['id']['videoId'] for item in search_response.get('items', [])]\n",
    "\n",
    "            if not video_ids:\n",
    "                continue\n",
    "\n",
    "            # fetch metadata\n",
    "            details_request = youtube_object.videos().list(\n",
    "                part='snippet,statistics',\n",
    "                id=','.join(video_ids)\n",
    "            )\n",
    "            details_response = details_request.execute()\n",
    "\n",
    "            for item in details_response.get('items', []):\n",
    "                snippet = item['snippet']\n",
    "                stats = item.get('statistics', {})\n",
    "                video_data = {\n",
    "                    'keyword': query,\n",
    "                    'video_id': item['id'],\n",
    "                    'title': snippet.get('title'),\n",
    "                    'published_at': snippet.get('publishedAt'),\n",
    "                    'channel_id': snippet.get('channelId'),\n",
    "                    'channel_title': snippet.get('channelTitle'),\n",
    "                    'view_count': int(stats.get('viewCount', 0)),\n",
    "                    'like_count': int(stats.get('likeCount', 0)),\n",
    "                    'comment_count': int(stats.get('commentCount', 0))\n",
    "                }\n",
    "                all_videos.append(video_data)\n",
    "\n",
    "            time.sleep(delay)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error while processing keyword '{query}': {e}\")\n",
    "            continue\n",
    "\n",
    "    return all_videos\n",
    "\n",
    "def fetch_youtube_comments_forest(youtube_object, video_ids):\n",
    "    \"\"\"\n",
    "    Fetch all comments from YouTube videos, structured like Reddit comment forests.\n",
    "\n",
    "    Args:\n",
    "        youtube_object: Authenticated YouTube API object.\n",
    "        video_ids (List[str]): List of YouTube video IDs.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of comment forests with hierarchical IDs.\n",
    "    \"\"\"\n",
    "    forests = []\n",
    "\n",
    "    def fetch_all_replies(parent_id):\n",
    "        replies = []\n",
    "        next_page_token = None\n",
    "        while True:\n",
    "            response = youtube_object.comments().list(\n",
    "                part=\"snippet\",\n",
    "                parentId=parent_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token\n",
    "            ).execute()\n",
    "            replies.extend(response.get(\"items\", []))\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "            time.sleep(2)\n",
    "        return replies\n",
    "\n",
    "    def index_comment_tree(comments, parent_id, prefix, depth):\n",
    "\n",
    "        indexed = []\n",
    "\n",
    "        for i, comment in enumerate(comments, start=1):\n",
    "\n",
    "            hier_id = f\"{prefix}{i}\" if prefix == \"\" else f\"{prefix}.{i}\"\n",
    "            comment_data = {\n",
    "                \"hier_id\": hier_id,\n",
    "                \"comment_id\": comment[\"id\"],\n",
    "                \"body\": comment[\"snippet\"][\"textDisplay\"],\n",
    "                \"like_count\": comment[\"snippet\"].get(\"likeCount\", 0),\n",
    "                \"parent_id\": parent_id,\n",
    "                \"depth\": depth,\n",
    "                \"author\": comment[\"snippet\"].get(\"authorDisplayName\"),\n",
    "                \"published_at\": comment[\"snippet\"].get(\"publishedAt\"),\n",
    "            }\n",
    "            indexed.append(comment_data)\n",
    "\n",
    "            # fetch replies if not present\n",
    "            reply_items = fetch_all_replies(comment[\"id\"])\n",
    "            if reply_items:\n",
    "                indexed.extend(index_comment_tree(\n",
    "                    reply_items,\n",
    "                    parent_id=comment[\"id\"],\n",
    "                    prefix=hier_id,\n",
    "                    depth=depth + 1\n",
    "                ))\n",
    "\n",
    "        return indexed\n",
    "\n",
    "    for video_id in video_ids:\n",
    "\n",
    "        all_comments = []\n",
    "        next_page_token = None\n",
    "\n",
    "        while True:\n",
    "            response = youtube_object.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token\n",
    "            ).execute()\n",
    "\n",
    "            threads = response.get(\"items\", [])\n",
    "\n",
    "            for thread in threads:\n",
    "                top_comment = thread[\"snippet\"][\"topLevelComment\"]\n",
    "\n",
    "\n",
    "                replies = thread.get(\"replies\", {}).get(\"comments\", [])\n",
    "                thread_data = {\n",
    "                    \"id\": top_comment[\"id\"],\n",
    "                    \"snippet\": top_comment[\"snippet\"],\n",
    "                    \"replies\": {\"comments\": replies} if replies else None\n",
    "                }\n",
    "                all_comments.append(thread_data)\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "            time.sleep(2)\n",
    "\n",
    "        # flatten and assign hier_ids\n",
    "        indexed_comments = index_comment_tree(\n",
    "            all_comments, \n",
    "            parent_id=f\"yt_{video_id}\", \n",
    "            prefix=\"\", \n",
    "            depth=0)\n",
    "\n",
    "        forests.append({\n",
    "            \"video_id\": video_id,\n",
    "            \"comments\": indexed_comments\n",
    "        })\n",
    "\n",
    "    return forests\n",
    "\n",
    "def fetch_all_youtube_comments(youtube_object, videos, log_path=\"youtube_comment_log.csv\", out_dir=\"youtube_comments\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # load existing log\n",
    "    completed_videos = set()\n",
    "    if os.path.exists(log_path):\n",
    "        with open(log_path, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                if row.get(\"success\") == \"True\":\n",
    "                    completed_videos.add(row[\"video_id\"])\n",
    "\n",
    "    # open log file for appending\n",
    "    log_file = open(log_path, \"a\", newline='', encoding=\"utf-8\")\n",
    "    log_writer = csv.DictWriter(log_file, fieldnames=[\"video_id\", \"success\", \"fetched_count\", \"expected_count\"])\n",
    "    if log_file.tell() == 0:\n",
    "        log_writer.writeheader()\n",
    "\n",
    "    for i, video in enumerate(videos):\n",
    "        video_id = video[\"video_id\"]\n",
    "        expected_count = int(video.get(\"comment_count\", 0))\n",
    "\n",
    "        if expected_count == 0:\n",
    "            print(f\"[SKIP] Video {video_id} has 0 expected comments.\")\n",
    "            log_writer.writerow({\n",
    "                \"video_id\": video_id,\n",
    "                \"success\": True,\n",
    "                \"fetched_count\": 0,\n",
    "                \"expected_count\": 0\n",
    "            })\n",
    "            log_file.flush()\n",
    "            continue\n",
    "\n",
    "        if video_id in completed_videos:\n",
    "            print(f\"[SKIP] Video {video_id} already processed.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"[{i+1}/{len(videos)}] Fetching comments for: {video_id}...\")\n",
    "\n",
    "        try:\n",
    "            result = fetch_youtube_comments_forest(youtube_object, video_ids=[video_id])\n",
    "            fetched_count = len(result[0][\"comments\"]) if result else 0\n",
    "\n",
    "            # save\n",
    "            with open(os.path.join(out_dir, f\"{video_id}.json\"), \"w\", encoding=\"utf-8\") as jf:\n",
    "                json.dump(result, jf, ensure_ascii=False, indent=2)\n",
    "\n",
    "            # log result\n",
    "            success = (fetched_count >= expected_count) if expected_count > 0 else True\n",
    "            log_writer.writerow({\n",
    "                \"video_id\": video_id,\n",
    "                \"success\": success,\n",
    "                \"fetched_count\": fetched_count,\n",
    "                \"expected_count\": expected_count\n",
    "            })\n",
    "            log_file.flush()\n",
    "\n",
    "            print(f\"Done. Fetched {fetched_count} / {expected_count} comments.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch for {video_id}: {e}\")\n",
    "            log_writer.writerow({\n",
    "                \"video_id\": video_id,\n",
    "                \"success\": False,\n",
    "                \"fetched_count\": 0,\n",
    "                \"expected_count\": expected_count\n",
    "            })\n",
    "            log_file.flush()\n",
    "            time.sleep(2)\n",
    "\n",
    "    log_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eac7a6",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ef2999",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"YT_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found in .env file\")\n",
    "\n",
    "youtube_object = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c6ce2f",
   "metadata": {},
   "source": [
    "### __Search videos__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb269b",
   "metadata": {},
   "source": [
    "#### Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4b2fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "greek_keywords = [\"ομόφυλα ζευγάρια\", \n",
    "                  \"ομόφυλα τεκνοθεσία\", \n",
    "                  \"ισότητα στο πολιτικό γάμο\", \n",
    "                  \"γάμος ομόφυλων\", \n",
    "                  \"γάμος ομόφυλων ζευγαριών\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a60136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the videos based on the keywords\n",
    "video_search_results = search_youtube_videos(\n",
    "    youtube_object=youtube_object,\n",
    "    keywords=greek_keywords,\n",
    "    region_code='GR', \n",
    "    language='el', \n",
    "    max_results=50 \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "222e8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data, duplicates = unique_posts_videos(video_search_results, id_key=\"video_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(video_search_results), len(clean_data), len(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f52188",
   "metadata": {},
   "source": [
    "#### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cd78a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(\"youtube_api.ipynb\")))) + \"\\\\outputs\\\\api_queried\\\\youtube_api\\\\youtube_scraped_videos.json\"\n",
    "\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(clean_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f682998",
   "metadata": {},
   "source": [
    "### __Search comments__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82468311",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(\"youtube_api.ipynb\")))) + \"\\\\outputs\\\\api_queried\\\\youtube_api\"\n",
    "log_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(\"youtube_api.ipynb\")))) + \"\\\\outputs\\\\logs\\\\youtube_comment_log.csv\"\n",
    "log_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0713e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(yt_path + \"\\\\youtube_scraped_videos.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    videos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c038c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids_iter = [video[\"video_id\"] for video in videos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = fetch_all_youtube_comments(youtube_object, \n",
    "                                      videos=videos, \n",
    "                                      out_dir=yt_path,\n",
    "                                      log_path=log_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab9ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
